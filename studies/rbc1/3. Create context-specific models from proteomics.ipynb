{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2537d45b-6871-43b9-81f8-e3639df15ac5",
   "metadata": {},
   "source": [
    "# Create context-specific models using proteomic data\n",
    "## Setup\n",
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2255cec1-1c92-48cd-b35a-01065975dc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import gurobipy as gp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib_venn as mpl_venn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sympy\n",
    "from cobra.flux_analysis import find_blocked_reactions\n",
    "from rbc_gem_utils import (\n",
    "    COBRA_CONFIGURATION,\n",
    "    GEM_NAME,\n",
    "    read_cobra_model,\n",
    "    show_versions,\n",
    "    write_cobra_model,\n",
    ")\n",
    "from rbc_gem_utils.analysis.overlay import (\n",
    "    ProteinDilution,\n",
    "    add_relaxation_budget,\n",
    "    load_overlay_model,\n",
    "    update_slack_value,\n",
    ")\n",
    "from rbc_gem_utils.util import AVOGADRO_NUMBER, DEFAULT_DRY_MASS_PER_CELL\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "gp.setParam(\"OutputFlag\", 0)\n",
    "gp.setParam(\"LogToConsole\", 0)\n",
    "\n",
    "# Show versions of notebook\n",
    "show_versions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1f0e62-ca1f-4201-a29d-dc3221be59c1",
   "metadata": {},
   "source": [
    "### Define configuration\n",
    "#### COBRA Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9d9475-8441-4f92-b46e-6257c5a7d843",
   "metadata": {},
   "outputs": [],
   "source": [
    "COBRA_CONFIGURATION.solver = \"gurobi\"\n",
    "COBRA_CONFIGURATION.bounds = (-1e8, 1e8)\n",
    "COBRA_CONFIGURATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dc6fa4-161e-4511-bf31-0b75d3b93dcb",
   "metadata": {},
   "source": [
    "## Load RBC-GEM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38de2e8f-e0f3-4e0c-94dd-52ea8edfb863",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"data\").resolve()\n",
    "models_path = Path(\"models\").resolve()\n",
    "figures_path = Path(\"figures\").resolve()\n",
    "\n",
    "dataset_name = \"RBComics\"\n",
    "dataset_path = Path(dataset_name).resolve()\n",
    "\n",
    "imagetype = \"svg\"\n",
    "transparent = True\n",
    "save_figures = True\n",
    "\n",
    "ftype = \"xml\"\n",
    "model = read_cobra_model(models_path / f\"{GEM_NAME.replace('-', '_')}.{ftype}\")\n",
    "pcmodel = load_overlay_model(filename=models_path / f\"{model.id}_PC.{ftype}\")\n",
    "\n",
    "pcmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a75d58-9789-4c76-a376-2db52ac44c2a",
   "metadata": {},
   "source": [
    "## Load RBC Proteomics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f773d2fe-5020-443a-b2dd-f3bbf9dffd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure index corresponds to sample IDs and columns are proteins IDs\n",
    "sample_prefix, time_prefix = (\"S\", \"D\")\n",
    "# Integers are easier to work with for time points\n",
    "timepoints = [10, 23, 42]\n",
    "df_copy_number_samples = pd.read_csv(\n",
    "    dataset_path / f\"{dataset_name}_CopyNumbers.tsv\", sep=\"\\t\", index_col=0\n",
    ").T\n",
    "sample_ids = df_copy_number_samples.index\n",
    "df_copy_number_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9625c447-4860-4b7b-a784-59a655d60045",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_from_id(model_id, sample_prefix=\"\"):\n",
    "    sample = model_id.rsplit(\"_\", 2)[-2]\n",
    "    try:\n",
    "        return int(sample.replace(sample_prefix, \"\"))\n",
    "    except ValueError:\n",
    "        return sample\n",
    "\n",
    "\n",
    "def get_time_from_id(model_id, time_prefix=\"\"):\n",
    "    time = model_id.rsplit(\"_\", 2)[-1]\n",
    "    try:\n",
    "        return int(time.replace(time_prefix, \"\"))\n",
    "    except ValueError:\n",
    "        return time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0398d3d-c9fc-4753-bb63-32f9f949847f",
   "metadata": {},
   "source": [
    "### Get data subsets using operations on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7168c70-1ce0-4427-aa7d-03ee44aa067d",
   "metadata": {},
   "outputs": [],
   "source": [
    "operations = [\n",
    "    \"mean\",\n",
    "    \"median\",\n",
    "    # \"min\",\n",
    "    # \"max\",\n",
    "]\n",
    "percentiles = []\n",
    "\n",
    "\n",
    "if operations or percentiles:\n",
    "    df_operations = []\n",
    "    for time in timepoints:\n",
    "        df_time = df_copy_number_samples.loc[\n",
    "            [\n",
    "                x\n",
    "                for x in df_copy_number_samples.index\n",
    "                if get_time_from_id(x, time_prefix) == time\n",
    "            ]\n",
    "        ].T\n",
    "        df_op = pd.concat(\n",
    "            [getattr(df_time, op.lower())(axis=1) for op in operations], axis=1\n",
    "        )\n",
    "        df_op.columns = [f\"{op.capitalize()}_{time_prefix}{time}\" for op in operations]\n",
    "        df_operations += [df_op]\n",
    "        if percentiles:\n",
    "            df_op = pd.concat(\n",
    "                [df_time.quantile(percent, axis=1) for percent in percentiles], axis=1\n",
    "            )\n",
    "            df_op.columns = [\n",
    "                f\"Percentile{int(round(percent * 100, 0))}_{time_prefix}{time}\"\n",
    "                for percent in percentiles\n",
    "            ]\n",
    "            df_operations += [df_op]\n",
    "    df_operations = pd.concat(df_operations, axis=1)\n",
    "    operation_ids = df_operations.columns\n",
    "    df_copy_number_all = pd.concat((df_copy_number_samples.T, df_operations), axis=1).T\n",
    "else:\n",
    "    df_copy_number_all = df_copy_number_samples.T\n",
    "    operation_ids = []\n",
    "df_copy_number_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f0f2b8-c950-44af-a911-124cd4b153db",
   "metadata": {},
   "source": [
    "## Integrate proteomics with model\n",
    "### Convert copy numbers to mg / gDW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c722b2f4-d349-4053-a6c7-95965361315c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_protein_data = pd.read_csv(\n",
    "    f\"{dataset_path}/{dataset_name}_protein_data.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    index_col=\"Entry\",\n",
    ")\n",
    "df_uniprot_to_mw = df_protein_data[\"Mass\"] / 1000  # g/mol --> # kg / mol\n",
    "\n",
    "df_mg_prot_per_gDW = (\n",
    "    df_copy_number_all  # protein copies / cell\n",
    "    * (1 / DEFAULT_DRY_MASS_PER_CELL)  # cell / pgDW\n",
    "    * (1e12 / 1)  # pgDW / gDW\n",
    "    * (1 / AVOGADRO_NUMBER)  # mol / protein copies\n",
    "    * (df_uniprot_to_mw)  # kg / mol\n",
    "    * (1e6 / 1)  # mg / kg\n",
    ").copy()\n",
    "df_mg_prot_per_gDW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4f99db-0ce3-4342-848f-03ddae9e4710",
   "metadata": {},
   "source": [
    "### Scale measurements for proteome budget\n",
    "Note that this step will help ensure its theoretically possible for a perfect fit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8d6fe5-54dc-4288-8127-4f066b47ef2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "proteome_budget_value = 50\n",
    "hemoglobin_budget_value = 950\n",
    "total_budget_value = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b51a37-eb71-43ed-9355-bebae6c6cfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into hemoglobin and low abundance proteomes\n",
    "hb_proteins = {\n",
    "    # Model Gene: UniProt ID\n",
    "    \"HBA\": \"P69905\",\n",
    "    \"HBB\": \"P68871\",\n",
    "    \"HBD\": \"P02042\",\n",
    "    \"HBM\": \"Q6B0K9\",\n",
    "    \"HBE1\": \"P02100\",\n",
    "    \"HBG1\": \"P69891\",\n",
    "    \"HBG2\": \"P69892\",\n",
    "    \"HBQ1\": \"P09105\",\n",
    "    \"HBZ\": \"P02008\",\n",
    "}\n",
    "df_mg_prot_per_gDW_hb = df_mg_prot_per_gDW.loc[:, list(hb_proteins.values())]\n",
    "df_mg_prot_per_gDW_la = df_mg_prot_per_gDW.loc[\n",
    "    :, [x for x in df_mg_prot_per_gDW.columns if not x in list(hb_proteins.values())]\n",
    "]\n",
    "\n",
    "PBDL_proteome_budget = pcmodel.reactions.get_by_id(\"PBDL_proteome_budget\")\n",
    "PBDL_hemoglobin_budget = pcmodel.reactions.get_by_id(\"PBDL_hemoglobin_budget\")\n",
    "PBDL_total_budget = pcmodel.reactions.get_by_id(\"PBDL_total_budget\")\n",
    "\n",
    "if proteome_budget_value is None:\n",
    "    proteome_budget_value = PBDL_proteome_budget.upper_bound\n",
    "if hemoglobin_budget_value is None:\n",
    "    hemoglobin_budget_value = PBDL_hemoglobin_budget.upper_bound\n",
    "if total_budget_value is None:\n",
    "    total_budget_value = PBDL_total_budget.upper_bound\n",
    "\n",
    "assert total_budget_value >= (proteome_budget_value + hemoglobin_budget_value)\n",
    "\n",
    "PBDL_proteome_budget.upper_bound = proteome_budget_value\n",
    "PBDL_hemoglobin_budget.upper_bound = hemoglobin_budget_value\n",
    "PBDL_total_budget.upper_bound = total_budget_value\n",
    "\n",
    "# Scale values for low abundance proteome\n",
    "budget_value = proteome_budget_value\n",
    "df_mg_prot_per_gDW_la = (\n",
    "    budget_value * (df_mg_prot_per_gDW_la.T / df_mg_prot_per_gDW_la.sum(axis=1)).T\n",
    ")\n",
    "print(f\"Low abundance budget:\\t{budget_value}\\t(={budget_value * 100 / 1000}%)\")\n",
    "\n",
    "# Scale values for hemoglobin proteome\n",
    "budget_value = hemoglobin_budget_value\n",
    "df_mg_prot_per_gDW_hb = (\n",
    "    budget_value * (df_mg_prot_per_gDW_hb.T / df_mg_prot_per_gDW_hb.sum(axis=1)).T\n",
    ")\n",
    "print(f\"Hemoglobin budget:\\t{budget_value}\\t(={budget_value * 100 / 1000}%)\")\n",
    "\n",
    "budget_value = total_budget_value - sum(\n",
    "    [proteome_budget_value, hemoglobin_budget_value]\n",
    ")\n",
    "print(f\"Remaining budget:\\t{budget_value}\\t(={budget_value * 100 / 1000}%)\")\n",
    "\n",
    "\n",
    "# Combine dataframes back into one\n",
    "df_mg_prot_per_gDW_normalized = pd.concat(\n",
    "    (df_mg_prot_per_gDW_hb, df_mg_prot_per_gDW_la), axis=1\n",
    ")\n",
    "df_mg_prot_per_gDW_normalized.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf2c529-58f3-4fce-9c7b-d41f11e2f09b",
   "metadata": {},
   "source": [
    "### Convert mg / gDW to nmol / gDW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b5ef09-c5a2-4985-b9c7-9d26d593d680",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nmol_prot_per_gDW = (\n",
    "    df_mg_prot_per_gDW_normalized  # mg / gDW\n",
    "    * (1 / df_uniprot_to_mw)  # mol / kg --> mmol / g --> umol / mg\n",
    "    * (1e3 / 1)  # nmol / umol\n",
    ").loc[:, df_mg_prot_per_gDW_normalized.columns]\n",
    "df_nmol_prot_per_gDW = df_nmol_prot_per_gDW.T\n",
    "df_nmol_prot_per_gDW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce81e95-0e7f-46a4-b1c8-b16e958155cd",
   "metadata": {},
   "source": [
    "## Create DataFrame for protein dilution reactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef7d9d3-4a84-4840-98f9-d080c6cf7137",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_protein_dilutions = pd.concat(\n",
    "    (\n",
    "        pd.Series(\n",
    "            {g.annotation.get(\"uniprot\"): g.id for g in model.genes}, name=\"genes\"\n",
    "        ),\n",
    "        pd.Series(\n",
    "            {\n",
    "                protdl.annotation.get(\"uniprot\"): protdl.id\n",
    "                for protdl in pcmodel.reactions.query(\n",
    "                    lambda x: isinstance(x, ProteinDilution)\n",
    "                )\n",
    "            },\n",
    "            name=\"PROTDL\",\n",
    "        ),\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "df_model_protein_dilutions.index.name = \"uniprot\"\n",
    "df_model_protein_dilutions = df_model_protein_dilutions[\n",
    "    df_model_protein_dilutions[\"genes\"].isin(model.genes.list_attr(\"id\"))\n",
    "].sort_values(\"PROTDL\")\n",
    "df_model_protein_dilutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99a5311-87f7-4d17-a0f2-9eef9c4b52d3",
   "metadata": {},
   "source": [
    "## Organize samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa68429c-7657-4f42-9393-9e72d430145b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_samples = df_nmol_prot_per_gDW.copy()\n",
    "\n",
    "# Best to sort by donor, if sample fails solving at any point, only need to redo the n_timepoints associated with sample.\n",
    "sort_samples_by_donor = True\n",
    "merge_key = \"uniprot\"\n",
    "run_computations = False\n",
    "\n",
    "# Use this line of code to determine which samples are used.\n",
    "# Useful for picking up where notebook may have prematurely stopped.\n",
    "# Set as 0 or less to use all samples\n",
    "start_from_sample_id = 0\n",
    "# Set as 0 or less to use only operation samples\n",
    "# end_on_sample_id = 0\n",
    "# Set as length of samples to use all samples\n",
    "end_on_sample_id = len(df_samples)\n",
    "\n",
    "# Use this line of code to determine which time points are used.\n",
    "# Comment out to keep all original points defined in samples\n",
    "timepoints = [10, 23, 42]\n",
    "df_samples.index.name = merge_key\n",
    "# Filter out time points\n",
    "df_samples = df_samples.loc[\n",
    "    :, [x for x in df_samples.columns if get_time_from_id(x, time_prefix) in timepoints]\n",
    "]\n",
    "# Filter out irrelevant samples\n",
    "df_samples = pd.concat(\n",
    "    (\n",
    "        df_samples.loc[\n",
    "            :,\n",
    "            [\n",
    "                x\n",
    "                for x in sample_ids\n",
    "                if start_from_sample_id\n",
    "                <= get_sample_from_id(x, sample_prefix)\n",
    "                <= end_on_sample_id\n",
    "            ],\n",
    "        ],\n",
    "        df_samples.loc[:, [x for x in df_samples.columns if x in operation_ids]],\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "if sort_samples_by_donor:\n",
    "    df_samples = pd.concat(\n",
    "        (\n",
    "            df_samples.loc[\n",
    "                :,\n",
    "                sorted(\n",
    "                    [x for x in df_samples.columns if x not in operation_ids],\n",
    "                    key=lambda x: (\n",
    "                        get_sample_from_id(x, sample_prefix),\n",
    "                        get_time_from_id(x, time_prefix),\n",
    "                    ),\n",
    "                ),\n",
    "            ],\n",
    "            df_samples.loc[\n",
    "                :,\n",
    "                sorted(\n",
    "                    [x for x in df_samples.columns if x in operation_ids],\n",
    "                    key=lambda x: get_time_from_id(x, time_prefix),\n",
    "                ),\n",
    "            ],\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "else:\n",
    "    df_samples = pd.concat(\n",
    "        [\n",
    "            pd.concat(\n",
    "                (\n",
    "                    df_samples.loc[\n",
    "                        :,\n",
    "                        sorted(\n",
    "                            [\n",
    "                                x\n",
    "                                for x in df_samples.columns\n",
    "                                if get_time_from_id(x, time_prefix) == time\n",
    "                                and x not in operation_ids\n",
    "                            ],\n",
    "                            key=lambda x: get_sample_from_id(x, sample_prefix),\n",
    "                        ),\n",
    "                    ],\n",
    "                    df_samples.loc[\n",
    "                        :,\n",
    "                        sorted(\n",
    "                            [\n",
    "                                x\n",
    "                                for x in df_samples.columns\n",
    "                                if get_time_from_id(x, time_prefix) == time\n",
    "                                and x in operation_ids\n",
    "                            ],\n",
    "                        ),\n",
    "                    ],\n",
    "                ),\n",
    "                axis=1,\n",
    "            )\n",
    "            for time in timepoints\n",
    "        ],\n",
    "        axis=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103fe128-1107-4bbb-a102-0477a5042fd4",
   "metadata": {},
   "source": [
    "### Map samples to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424fac1a-e134-438d-942e-d04e61bf5c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = (\n",
    "    df_model_protein_dilutions[[\"PROTDL\"]]\n",
    "    .merge(df_samples, left_index=True, right_index=True, how=\"left\")\n",
    "    .set_index(\"PROTDL\")\n",
    "    .sort_index()\n",
    ")\n",
    "no_experimental_measurements = [\n",
    "    protein_dilution\n",
    "    for protein_dilution, has_measurement in df_model.isna().all(axis=1).items()\n",
    "    if has_measurement\n",
    "]\n",
    "print(\n",
    "    f\"Model proteins mapped to measurements: {len(df_model) - len(no_experimental_measurements)}\"\n",
    ")\n",
    "print(f\"Model proteins without measurements: {len(no_experimental_measurements)}\")\n",
    "df_model.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ef46b1-6eaa-4ded-a10c-2595910014dd",
   "metadata": {},
   "source": [
    "#### Summarize mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f95b663-a988-4a69-9c84-78436c7252d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_proteins = set(df_samples.index)\n",
    "model_proteins = set(df_model_protein_dilutions.index)\n",
    "\n",
    "df_mg_prot_per_gDW_hb = df_mg_prot_per_gDW_normalized.loc[:, list(hb_proteins.values())]\n",
    "df_mg_prot_per_gDW_la = df_mg_prot_per_gDW_normalized.loc[\n",
    "    :, [x for x in df_mg_prot_per_gDW.columns if not x in list(hb_proteins.values())]\n",
    "]\n",
    "\n",
    "df_mapped_mass_la = df_mg_prot_per_gDW_la.loc[\n",
    "    :, df_mg_prot_per_gDW_la.columns.isin(model_proteins)\n",
    "].sum(axis=1)\n",
    "df_unmapped_mass_la = df_mg_prot_per_gDW_la.loc[\n",
    "    :, ~df_mg_prot_per_gDW_la.columns.isin(model_proteins)\n",
    "].sum(axis=1)\n",
    "df_mapped_mass_hb = df_mg_prot_per_gDW_hb.loc[\n",
    "    :, df_mg_prot_per_gDW_hb.columns.isin(model_proteins)\n",
    "].sum(axis=1)\n",
    "df_unmapped_mass_hb = df_mg_prot_per_gDW_hb.loc[\n",
    "    :, ~df_mg_prot_per_gDW_hb.columns.isin(model_proteins)\n",
    "].sum(axis=1)\n",
    "\n",
    "proteomes = {}\n",
    "round_int = 6\n",
    "for label, df in zip(\n",
    "    [\"hemoglobin\", \"low abundance\"], [df_mg_prot_per_gDW_hb, df_mg_prot_per_gDW_la]\n",
    "):\n",
    "    df_modeled = df.loc[:, df.columns.isin(model_proteins)].sum(axis=1)\n",
    "    df_remaining = df.loc[:, ~df.columns.isin(model_proteins)].sum(axis=1)\n",
    "    means = (df_modeled.mean(), df_remaining.mean())\n",
    "    stdevs = (df_modeled.std(), df_remaining.std())\n",
    "    proteomes[(label, \"modeled\")] = round(means[0], round_int)\n",
    "    proteomes[(label, \"remaining\")] = round(means[1], round_int)\n",
    "proteomes = pd.Series(proteomes, name=\"Mean value across samples\")\n",
    "proteomes.index = [f\"Mean {k[0]} mass {k[1]}\" for k in proteomes.index]\n",
    "print(proteomes.head())\n",
    "proteomes = proteomes[proteomes != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff826fe-24f6-4888-9f05-6853dce01624",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(3, 6))\n",
    "subsets = (\n",
    "    len(dataset_proteins),\n",
    "    len(model_proteins),\n",
    "    len(dataset_proteins.intersection(model_proteins)),\n",
    ")\n",
    "\n",
    "\n",
    "venn = mpl_venn.venn2(\n",
    "    subsets=subsets,\n",
    "    set_labels=(dataset_name, GEM_NAME),\n",
    "    set_colors=(\"red\", \"blue\"),\n",
    "    alpha=0.5,\n",
    "    ax=ax1,\n",
    ")\n",
    "circles = mpl_venn.venn2_circles(\n",
    "    subsets=subsets, linestyle=\"-\", color=\"black\", ax=ax1, linewidth=1\n",
    ")\n",
    "for text in venn.set_labels:\n",
    "    text.set_fontsize(\"x-large\")\n",
    "for text in venn.subset_labels:\n",
    "    text.set_fontsize(\"x-large\")\n",
    "ax1.set_title(\"Modeled proteome\", fontsize=\"xx-large\")\n",
    "\n",
    "\n",
    "label_color_map = {\n",
    "    \"Mean hemoglobin mass modeled\": (\"Hemoglobin\", \"xkcd:dark red\"),\n",
    "    \"Mean low abundance mass modeled\": (\"Low abundance\", \"xkcd:light blue\"),\n",
    "    \"Mean low abundance mass remaining\": (\"Not modeled\", \"xkcd:green\"),\n",
    "}\n",
    "edgecolor = \"black\"\n",
    "linewidth = 1\n",
    "ax2.pie(\n",
    "    x=proteomes.values,\n",
    "    colors=[label_color_map[k][1] for k in proteomes.index],\n",
    "    pctdistance=1.35,\n",
    "    counterclock=False,\n",
    "    autopct=lambda pct: f\"{pct * 1000/100:.2f}\\n\",\n",
    "    textprops=dict(fontsize=\"large\", ha=\"center\", va=\"top\"),\n",
    "    wedgeprops=dict(edgecolor=edgecolor, linewidth=linewidth),\n",
    ")\n",
    "handles = [\n",
    "    mpl.patches.Patch(\n",
    "        edgecolor=edgecolor,\n",
    "        linewidth=linewidth,\n",
    "        label=label_color_map[k][0],\n",
    "        facecolor=label_color_map[k][1],\n",
    "    )\n",
    "    for k in proteomes.index\n",
    "]\n",
    "ax2.legend(\n",
    "    handles=handles,\n",
    "    ncols=1,\n",
    "    bbox_to_anchor=(0.5, 0),\n",
    "    loc=\"upper center\",\n",
    "    fontsize=\"large\",\n",
    "    frameon=False,\n",
    ")\n",
    "ax2.set_xlabel(\"Mass (mg/gDW)\", fontsize=\"large\", labelpad=-10)\n",
    "fig.tight_layout()\n",
    "if save_figures:\n",
    "    fig.savefig(\n",
    "        figures_path / f\"Fig5_PanelEF_ModeledProteome.{imagetype}\",\n",
    "        transparent=transparent,\n",
    "        format=imagetype,\n",
    "    )\n",
    "fig;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37570a6b-a582-488e-ae00-426e50c9bec6",
   "metadata": {},
   "source": [
    "### Reduce model using maximum values from proteomics\n",
    "Create a reduced model using the maximum value measured of each protein across all data samples. By finding the reactions that always remain blocked prior to analysis, the computation time may be reduced.\n",
    "Also ensure that proteins not found in the dataset don't significantly sway results,\n",
    "\n",
    "Note: May be problematic if proteins essential for catalysis are not measured. Keep protein bounds unchanged to remove only normal blocked reactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb9510e-9eca-4a3e-b78b-357eae3344c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_blocked_reactions = False\n",
    "\n",
    "if remove_blocked_reactions:\n",
    "    pcmodel_blocked = pcmodel.copy()\n",
    "    df = df_model.max(axis=1)\n",
    "    for protein_dilution, bound in df.items():\n",
    "        protein_dilution = pcmodel_blocked.reactions.get_by_id(protein_dilution)\n",
    "        # if not np.isnan(bound):\n",
    "        #     protein_dilution.bounds = (0, bound)\n",
    "        # else:\n",
    "        #     protein_dilution.bounds = (0, 0)\n",
    "    blocked = find_blocked_reactions(pcmodel_blocked)\n",
    "    pcmodel.remove_reactions(blocked, remove_orphans=True)\n",
    "\n",
    "for objective_reaction in [\"NaKt\"]:\n",
    "    objective_reaction = pcmodel.reactions.get_by_id(objective_reaction)\n",
    "    pcmodel.objective = objective_reaction.flux_expression\n",
    "    print(pcmodel.slim_optimize())\n",
    "\n",
    "pcmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b212aa-2689-4cd1-b5b8-08c57ad6aaba",
   "metadata": {},
   "source": [
    "## Create QP model for each sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4904c726-a9d6-4890-a74f-09a6fd3b9e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_qp(pcmodel, df):\n",
    "    x = []  # Variables\n",
    "    c = []  # Data * Weights\n",
    "    F = []  # Weights\n",
    "\n",
    "    for protdl, (data_value, weight) in df.iterrows():\n",
    "        protdl = pcmodel.reactions.get_by_id(protdl)\n",
    "        x.append(protdl.flux_expression)\n",
    "        c.append(weight * data_value)\n",
    "        F.append(weight)\n",
    "\n",
    "    x = sympy.Matrix(x)\n",
    "    c = sympy.Matrix(c)\n",
    "    F = sympy.DiagMatrix(sympy.Matrix(F))\n",
    "    # # QP Objective must be in form of 0.5 * x.T * F * x - c.T * x\n",
    "    objective = 0.5 * x.T * F * x - c.T * x\n",
    "    pcmodel.objective = objective[0]\n",
    "    pcmodel.objective_direction = \"min\"\n",
    "    pcmodel.tolerance = 1e-9\n",
    "\n",
    "    qp_sol = pcmodel.optimize()\n",
    "    return qp_sol\n",
    "\n",
    "\n",
    "def solve_qp_for_samples(pcmodel, df_samples, df_weights=None, verbose=False):\n",
    "    qp_solutions_dict = {}\n",
    "    for sample_id, data_series in df_samples.items():\n",
    "        # Get protein values\n",
    "        data_series.name = \"Data\"\n",
    "        if df_weights is None:\n",
    "            data_weights = 1 / data_series.replace(0, 1)\n",
    "            data_weights = data_weights / data_weights.mean()\n",
    "        else:\n",
    "            data_weights = df_weights.loc[:, sample_id]\n",
    "        # Get protein weights\n",
    "        data_weights.name = \"Weights\"\n",
    "\n",
    "        # Map to model, currently model mapping DataFrame generated outside scope of function\n",
    "        df_model_data_weights = (\n",
    "            df_model_protein_dilutions[[\"PROTDL\"]]\n",
    "            .merge(data_series, left_index=True, right_index=True, how=\"left\")\n",
    "            .merge(data_weights, left_index=True, right_index=True, how=\"left\")\n",
    "            .set_index(\"PROTDL\")\n",
    "            .sort_index()\n",
    "        )\n",
    "\n",
    "        df = df_model_data_weights.loc[:, [data_series.name, data_weights.name]].dropna(\n",
    "            axis=0, how=\"all\"\n",
    "        )\n",
    "\n",
    "        with pcmodel:\n",
    "            qp_sol = solve_qp(pcmodel, df)\n",
    "\n",
    "        df_qp_sol = qp_sol.fluxes.loc[\n",
    "            pcmodel.reactions.query(lambda x: isinstance(x, ProteinDilution)).list_attr(\n",
    "                \"id\"\n",
    "            )\n",
    "        ]\n",
    "        df_qp_sol = (\n",
    "            pd.concat((df_model_data_weights, df_qp_sol), axis=1).dropna().sort_index()\n",
    "        )\n",
    "        # data_weights = df_qp_sol.loc[:, \"Weights\"]\n",
    "\n",
    "        df_qp_sol = df_qp_sol.rename(\n",
    "            {\"Data\": \"Measured Proteome\", \"fluxes\": \"Best-Fitted Proteome\"}, axis=1\n",
    "        )\n",
    "        df_qp_sol = df_qp_sol.loc[:, [\"Measured Proteome\", \"Best-Fitted Proteome\"]]\n",
    "\n",
    "        r2 = r2_score(\n",
    "            df_qp_sol.iloc[:, 0].values,\n",
    "            df_qp_sol.iloc[:, 1].values,\n",
    "            multioutput=\"uniform_average\",\n",
    "        )\n",
    "        qp_solutions_dict[sample_id] = (df_qp_sol, r2, qp_sol.objective_value)\n",
    "        if verbose:\n",
    "            # Recall that the objective is designed to try to minimize fitting error via maximizing R2, so 1 is a possibility\n",
    "            print(\n",
    "                \"R^2 (objective) value for Sample '{}': {:.9f} ({:.5f})\".format(\n",
    "                    sample_id, r2, qp_sol.objective_value\n",
    "                )\n",
    "            )\n",
    "        # TODO catch bad fits\n",
    "\n",
    "    return qp_solutions_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c865bd-1409-400d-9e1f-d8ad5c9c9093",
   "metadata": {},
   "source": [
    "### Set weightings for QP problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9a9e24-5841-476b-84fd-cfe3d48237aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure data is provided as (Protein IDs x Sample IDs)\n",
    "# Use original measurements for weights\n",
    "df_weights = df_copy_number_all.T.loc[:, df_samples.columns]\n",
    "df_weights = 1 / df_weights.replace(0, 1)\n",
    "df_weights /= df_weights.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7609232-c234-4f93-b438-d3cfbe8d36dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_computations:\n",
    "    qp_solutions_dict = solve_qp_for_samples(\n",
    "        pcmodel, df_samples, df_weights=df_weights, verbose=True\n",
    "    )\n",
    "    df_measured = {}\n",
    "    df_best_fit = {}\n",
    "    df_r2_objective = {}\n",
    "    for sample_id, (df_qp_sol, r2, objective_value) in qp_solutions_dict.items():\n",
    "        df_measured[sample_id] = df_qp_sol[\"Measured Proteome\"].to_dict()\n",
    "        df_best_fit[sample_id] = df_qp_sol[\"Best-Fitted Proteome\"].to_dict()\n",
    "        df_r2_objective[sample_id] = {\"R2\": r2, \"Objective\": objective_value}\n",
    "\n",
    "    df_measured = pd.DataFrame.from_dict(df_measured, orient=\"columns\")\n",
    "    df_measured.to_csv(f\"{dataset_path}/proteome_measured.tsv\", sep=\"\\t\", index=True)\n",
    "\n",
    "    df_best_fit = pd.DataFrame.from_dict(df_best_fit, orient=\"columns\")\n",
    "    df_best_fit.to_csv(f\"{dataset_path}/proteome_best_fit.tsv\", sep=\"\\t\", index=True)\n",
    "\n",
    "    df_r2_objective = pd.DataFrame.from_dict(df_r2_objective, orient=\"columns\")\n",
    "    df_r2_objective.to_csv(\n",
    "        f\"{dataset_path}/proteome_r2_objective.tsv\", sep=\"\\t\", index=True\n",
    "    )\n",
    "else:\n",
    "    df_measured = pd.read_csv(\n",
    "        f\"{dataset_path}/proteome_measured.tsv\", sep=\"\\t\", index_col=0\n",
    "    )\n",
    "    df_best_fit = pd.read_csv(\n",
    "        f\"{dataset_path}/proteome_best_fit.tsv\", sep=\"\\t\", index_col=0\n",
    "    )\n",
    "    df_r2_objective = pd.read_csv(\n",
    "        f\"{dataset_path}/proteome_r2_objective.tsv\", sep=\"\\t\", index_col=0\n",
    "    )\n",
    "\n",
    "    qp_solutions_dict = {}\n",
    "    for sample_id in list(sample_ids) + list(operation_ids):\n",
    "        if not sample_id.replace(f\"{pcmodel.id}_\", \"\") in df_samples.columns:\n",
    "            continue\n",
    "        df_qp_sol = pd.concat(\n",
    "            (\n",
    "                df_measured.loc[:, sample_id],\n",
    "                df_best_fit.loc[:, sample_id],\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "        df_qp_sol.columns = [\"Measured Proteome\", \"Best-Fitted Proteome\"]\n",
    "        r2, objective_value = df_r2_objective.loc[:, sample_id].values\n",
    "        qp_solutions_dict[sample_id] = (df_qp_sol, r2, objective_value)\n",
    "print(f\"Number of QP solutions: {len(qp_solutions_dict)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8846cf-1031-4335-b50c-bc97a67b0573",
   "metadata": {},
   "source": [
    "### Plot fitting for the mean and median samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2185fc86-7702-44f6-b293-95ab659d9d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_to_plot = np.array(\n",
    "    [\n",
    "        # Best for 1 or 3 columns\n",
    "        [x for x in operation_ids if \"Mean\" in x],\n",
    "        [x for x in operation_ids if \"Median\" in x],\n",
    "    ]\n",
    ")\n",
    "\n",
    "r2_text_loc = \"lower right\"\n",
    "\n",
    "length = 4\n",
    "nrows, ncols = samples_to_plot.shape\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=nrows,\n",
    "    ncols=ncols,\n",
    "    figsize=(length * ncols, length * nrows),\n",
    "    sharex=True,\n",
    "    sharey=True,\n",
    ")\n",
    "sns.despine(fig)\n",
    "\n",
    "for idx, (sample_id, ax) in enumerate(zip(samples_to_plot.flatten(), axes.flatten())):\n",
    "    df_qp_sol, r2, objective_value = qp_solutions_dict[sample_id]\n",
    "    xlabel, ylabel = df_qp_sol.columns\n",
    "\n",
    "    ticks = np.geomspace(1e-6, 1e6, 5)\n",
    "    perfect_fit_line = ax.plot(\n",
    "        [0, ticks[-1]],\n",
    "        [0, ticks[-1]],\n",
    "        linestyle=\":\",\n",
    "        color=\"black\",\n",
    "        linewidth=1,\n",
    "        alpha=1,\n",
    "    )\n",
    "\n",
    "    df_zeros = df_qp_sol[(df_qp_sol.apply(lambda x: np.isclose(x, 0))).any(axis=1)]\n",
    "    df_perfect = df_qp_sol[\n",
    "        np.isclose(\n",
    "            abs(df_qp_sol[\"Measured Proteome\"] - df_qp_sol[\"Best-Fitted Proteome\"]), 0\n",
    "        )\n",
    "    ]\n",
    "    df_perfect = df_perfect[~df_perfect.index.isin(df_zeros.index)]\n",
    "\n",
    "    df_altered = df_qp_sol[\n",
    "        ~np.isclose(\n",
    "            abs(df_qp_sol[\"Measured Proteome\"] - df_qp_sol[\"Best-Fitted Proteome\"]), 0\n",
    "        )\n",
    "    ]\n",
    "    df_altered = df_altered[~df_altered.index.isin(df_zeros.index)]\n",
    "    df_always_zero = df_zeros[(df_zeros == 0).all(axis=1)]\n",
    "    df_zeros = df_zeros[~df_zeros.index.isin(df_always_zero.index)]\n",
    "    df_from_zeros = df_zeros[np.isclose(df_zeros[\"Measured Proteome\"], 0)]\n",
    "    df_to_zeros = df_zeros[np.isclose(df_zeros[\"Best-Fitted Proteome\"], 0)]\n",
    "\n",
    "    handles = [\n",
    "        ax.scatter(\n",
    "            data=df_perfect.replace(0, ticks[0]),\n",
    "            x=xlabel,\n",
    "            y=ylabel,\n",
    "            color=\"xkcd:blue\",\n",
    "            alpha=0.5,\n",
    "            edgecolors=\"black\",\n",
    "            linewidths=1,\n",
    "        ),\n",
    "        ax.scatter(\n",
    "            data=df_altered.replace(0, ticks[0]),\n",
    "            x=xlabel,\n",
    "            y=ylabel,\n",
    "            color=\"xkcd:yellow\",\n",
    "            alpha=0.5,\n",
    "            edgecolors=\"black\",\n",
    "            linewidths=1,\n",
    "        ),\n",
    "        ax.scatter(\n",
    "            data=df_from_zeros.replace(0, ticks[0]),\n",
    "            x=xlabel,\n",
    "            y=ylabel,\n",
    "            color=\"xkcd:green\",\n",
    "            alpha=0.5,\n",
    "            edgecolors=\"black\",\n",
    "            linewidths=1,\n",
    "        ),\n",
    "        ax.scatter(\n",
    "            data=df_to_zeros.replace(0, ticks[0]),\n",
    "            x=xlabel,\n",
    "            y=ylabel,\n",
    "            color=\"xkcd:red\",\n",
    "            alpha=0.5,\n",
    "            edgecolors=\"black\",\n",
    "            linewidths=1,\n",
    "        ),\n",
    "        ax.scatter(\n",
    "            data=df_always_zero.replace(0, ticks[0]),\n",
    "            x=xlabel,\n",
    "            y=ylabel,\n",
    "            color=\"xkcd:black\",\n",
    "            alpha=0.5,\n",
    "            edgecolors=\"black\",\n",
    "            linewidths=1,\n",
    "        ),\n",
    "    ]\n",
    "    labels = [\n",
    "        f\"Perfect fit\",\n",
    "        f\"Adjusted abundance\",\n",
    "        f\"Unexpressed to expressed\",\n",
    "        f\"Expressed to unexpressed\",\n",
    "        f\"Never expressed\",\n",
    "    ]\n",
    "\n",
    "    donor, day = sample_id.split(\"_\")\n",
    "    if not (\"Mean\" == donor or \"Median\" == donor):\n",
    "        donor = donor.replace(\"S\", \"; Donor \")\n",
    "    day = day.replace(\"D\", \"Day \")\n",
    "    fancy_sample_id = \" \".join((day, donor))\n",
    "\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_yscale(\"log\")\n",
    "\n",
    "    fontdict = {\"size\": \"xx-large\"}\n",
    "    if idx == len(samples_to_plot.flatten()) - np.ceil(ncols / 2):\n",
    "        ax.set_xlabel(xlabel, fontdict=fontdict)\n",
    "    fig.legend(\n",
    "        handles=handles,\n",
    "        labels=labels,\n",
    "        loc=\"lower center\",\n",
    "        ncols=len(labels),\n",
    "        frameon=False,\n",
    "        fontsize=\"large\",\n",
    "        markerscale=2,\n",
    "        bbox_to_anchor=(0.5, -0.05),\n",
    "    )\n",
    "    if idx % ncols == 0:\n",
    "        ax.set_ylabel(ylabel, fontdict=fontdict)\n",
    "\n",
    "    ax.set_xticks(ticks)\n",
    "    ax.set_yticks(ticks)\n",
    "\n",
    "    ax.xaxis.set_tick_params(labelsize=\"x-large\")\n",
    "    ax.yaxis.set_tick_params(labelsize=\"x-large\")\n",
    "\n",
    "    r2_format = \" = {:.4f}\"\n",
    "    if r2_text_loc == \"lower right\":\n",
    "        ax.text(\n",
    "            0.95,\n",
    "            0.1,\n",
    "            r\"$R^{2}$\" + r2_format.format(r2) + (\"\\n{}\".format(fancy_sample_id)),\n",
    "            transform=ax.transAxes,\n",
    "            color=\"black\",\n",
    "            fontsize=\"large\",\n",
    "            ha=\"right\",\n",
    "        )\n",
    "    elif r2_text_loc == \"upper left\":\n",
    "        ax.text(\n",
    "            0.05,\n",
    "            0.9,\n",
    "            (\"{}\\n\".format(fancy_sample_id)) + r\"$R^{2}$\" + r2_format.format(r2),\n",
    "            transform=ax.transAxes,\n",
    "            color=\"black\",\n",
    "            fontsize=\"large\",\n",
    "            ha=\"left\",\n",
    "        )\n",
    "    else:\n",
    "        pass\n",
    "fig.tight_layout()\n",
    "if save_figures:\n",
    "    fig.savefig(\n",
    "        figures_path / f\"S2Fig_Panel_QPfitting.{imagetype}\",\n",
    "        transparent=transparent,\n",
    "        format=imagetype,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb3095c-dd82-45c1-b076-c51d9f87b200",
   "metadata": {},
   "source": [
    "### Determine best value for slack variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9031c0-89d3-474a-800d-0138c7ce341c",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_pcmodels = []\n",
    "verbose = True\n",
    "objective_rxns = [\"NaKt\"]\n",
    "\n",
    "slack_min = 1e-5  # Slack %\n",
    "slack_max = 1.5\n",
    "if run_computations:\n",
    "    for sample_id in operation_ids:\n",
    "        df_qp_sol, r2, objective_value = qp_solutions_dict[sample_id]\n",
    "        # Create a copy of the model\n",
    "        pcmodel_sample = pcmodel.copy()\n",
    "        pcmodel_sample.id = f\"{pcmodel.id}_{sample_id}\"\n",
    "        for protdl in pcmodel_sample.reactions.query(\n",
    "            lambda x: isinstance(x, ProteinDilution)\n",
    "        ):\n",
    "            if protdl.id in df_qp_sol.index:\n",
    "                prot_bound = df_qp_sol.loc[protdl.id][\"Best-Fitted Proteome\"]\n",
    "            else:\n",
    "                prot_bound = 0\n",
    "            protdl.bounds = (float(prot_bound), float(prot_bound))\n",
    "        # Add the relaxation budget with slack = 0 first\n",
    "        add_relaxation_budget(pcmodel_sample, 0, verbose)\n",
    "        list_of_pcmodels += [pcmodel_sample]\n",
    "\n",
    "    solutions = {\n",
    "        pcmodel_sample.id: {\n",
    "            \"model\": [],\n",
    "            \"slack\": [],\n",
    "            \"objective\": [],\n",
    "            \"relaxation\": [],\n",
    "            \"_\".join(objective_rxns): [],\n",
    "        }\n",
    "        for pcmodel_sample in list_of_pcmodels\n",
    "    }\n",
    "    slack_min = 1e-5\n",
    "    slack_max = 1.5\n",
    "    for slack_value in np.geomspace(slack_min, slack_max, 251):\n",
    "        print(f\"Updating slack variable to {100 * slack_value:.4f}%.\")\n",
    "        for pcmodel_sample in list_of_pcmodels:\n",
    "            update_slack_value(pcmodel_sample, slack_value, verbose=False)\n",
    "            relaxation_demand = pcmodel_sample.reactions.get_by_id(\n",
    "                f\"PBDL_relaxation_budget\"\n",
    "            )\n",
    "            pcmodel_sample.objective = (\n",
    "                sum(\n",
    "                    [\n",
    "                        r.flux_expression\n",
    "                        for r in pcmodel_sample.reactions.get_by_any(objective_rxns)\n",
    "                    ]\n",
    "                )\n",
    "                - relaxation_demand.flux_expression\n",
    "            )\n",
    "            pcmodel_sample.objective_direction = \"max\"\n",
    "            sol = pcmodel_sample.optimize()\n",
    "            obj_value = sol.objective_value\n",
    "            if not obj_value or np.isnan(obj_value):\n",
    "                continue\n",
    "            else:\n",
    "                demand = relaxation_demand.flux\n",
    "                budget = relaxation_demand.upper_bound\n",
    "            solutions[pcmodel_sample.id][\"model\"].append(pcmodel_sample.id)\n",
    "            solutions[pcmodel_sample.id][\"slack\"].append(slack_value)\n",
    "            solutions[pcmodel_sample.id][\"objective\"].append(obj_value)\n",
    "            solutions[pcmodel_sample.id][\"_\".join(objective_rxns)].append(\n",
    "                obj_value + demand\n",
    "            )\n",
    "            solutions[pcmodel_sample.id][\"relaxation\"].append(demand / budget)\n",
    "    solutions = {\n",
    "        pcmodel_sample: pd.DataFrame.from_dict(sol)\n",
    "        for pcmodel_sample, sol in solutions.items()\n",
    "    }\n",
    "\n",
    "    df_relaxation = pd.concat(list(solutions.values()), axis=0)\n",
    "    df_relaxation.to_csv(\n",
    "        f\"{dataset_path}/SlackPercentDeterminationData_.tsv\", sep=\"\\t\", index=False\n",
    "    )\n",
    "else:\n",
    "    df_relaxation = pd.read_csv(\n",
    "        f\"{dataset_path}/SlackPercentDeterminationData.tsv\", sep=\"\\t\", index_col=None\n",
    "    )\n",
    "    solutions = {\n",
    "        mid: df_relaxation[df_relaxation[\"model\"] == mid].drop(\"model\", axis=1)\n",
    "        for mid in df_relaxation[\"model\"].unique()\n",
    "    }\n",
    "df_relaxation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8d5690-9261-4e16-b079-26324cd3eb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(\n",
    "    3, 1, figsize=(4, 10), sharex=True, gridspec_kw=dict(hspace=0.05)\n",
    ")\n",
    "axes = axes.flatten()\n",
    "\n",
    "# ax3d = fig.add_subplot(2, 2, 4, projection=\"3d\")\n",
    "sns.despine(fig)\n",
    "\n",
    "handles = []\n",
    "labels = []\n",
    "chosen_slack_var = 0.03\n",
    "colors = {\n",
    "    \"D10\": \"xkcd:green\",\n",
    "    \"D23\": \"xkcd:gold\",\n",
    "    \"D42\": \"xkcd:red\",\n",
    "}\n",
    "linestyles = {\n",
    "    \"Mean\": \"-\",\n",
    "    \"Median\": \"-.\",\n",
    "}\n",
    "use_percents = True\n",
    "for pcmodel_sample in list(solutions):\n",
    "    donor, day = str(pcmodel_sample).split(\"_\")[-2:]\n",
    "    linestyle = linestyles.get(donor, \":\")\n",
    "    color = colors.get(day, \"xkcd:light blue\")\n",
    "    if not (\"Mean\" == donor or \"Median\" == donor):\n",
    "        donor = donor.replace(\"S\", \"; Donor \")\n",
    "    day = day.replace(\"D\", \"Day \")\n",
    "    fancy_sample_id = \" \".join((day, donor))\n",
    "\n",
    "    labels.append(fancy_sample_id)\n",
    "    s_values = solutions[str(pcmodel_sample)][\"slack\"].values\n",
    "    r_values = solutions[str(pcmodel_sample)][\"relaxation\"].values * (\n",
    "        100 if use_percents else 1\n",
    "    )\n",
    "    o_values = solutions[str(pcmodel_sample)][\"objective\"].values\n",
    "    rxn_values = solutions[str(pcmodel_sample)][\"_\".join(objective_rxns)].values\n",
    "\n",
    "    zorder = 1\n",
    "    lw = 2\n",
    "    axes[0].plot(\n",
    "        s_values,\n",
    "        r_values,\n",
    "        label=str(pcmodel_sample),\n",
    "        color=color,\n",
    "        linestyle=linestyle,\n",
    "        linewidth=lw,\n",
    "        zorder=zorder,\n",
    "    )\n",
    "    axes[1].plot(\n",
    "        s_values,\n",
    "        o_values,\n",
    "        label=str(pcmodel_sample),\n",
    "        color=color,\n",
    "        linestyle=linestyle,\n",
    "        linewidth=lw,\n",
    "        zorder=zorder,\n",
    "    )\n",
    "    axes[2].plot(\n",
    "        s_values,\n",
    "        rxn_values,\n",
    "        label=str(pcmodel_sample),\n",
    "        color=color,\n",
    "        linestyle=linestyle,\n",
    "        linewidth=lw,\n",
    "        zorder=zorder,\n",
    "    )\n",
    "\n",
    "    # index = list(s_values).index(s_values[s_values >= chosen_slack_var][0])\n",
    "    # spt = s_values[index]\n",
    "    # rpt = r_values[index]\n",
    "    # opt = o_values[index]\n",
    "    # rxnpt = rxn_values[index]\n",
    "    # c = \"black\"\n",
    "    # ls = \"\"\n",
    "    # marker = \"o\"\n",
    "    # markersize = 8\n",
    "    # axes[0].plot(spt, rpt, color=c, linestyle=ls, marker=marker, markersize=markersize)\n",
    "    # axes[1].plot(spt, opt, color=c, linestyle=ls, marker=marker, markersize=markersize)\n",
    "    # axes[2].plot(spt, rxnpt, color=c, linestyle=ls, marker=marker, markersize=markersize)\n",
    "    # ax3d.plot(spt, rpt, opt, color=c, linestyle=ls, marker=marker, markersize=markersize)\n",
    "    # print(f\"Elbow point for {pcmodel_sample}: ({spt:.5f}, {rpt:.5f}, {opt:.5f})\")\n",
    "fontdict = {\"size\": \"x-large\"}\n",
    "axes[-1].set_xlabel(r\"Slack variable $s$\", fontdict=fontdict)\n",
    "\n",
    "i = 0\n",
    "zorder = 2\n",
    "alpha = 0.7\n",
    "limit_pad_sclar = 1.2\n",
    "ymin, ymax = (-0.001 * (100 if use_percents else 1), max(r_values) * limit_pad_sclar)\n",
    "smin = s_values[list(o_values).index(o_values[o_values <= 0][0])]\n",
    "axes[i].vlines(chosen_slack_var, ymin=ymin, ymax=ymax, color=\"black\", linestyle=\":\")\n",
    "axes[i].vlines(\n",
    "    smin, ymin=ymin, ymax=ymax, color=\"black\", linestyle=\"-\", zorder=zorder, alpha=alpha\n",
    ")\n",
    "axes[i].vlines(\n",
    "    1, ymin=ymin, ymax=ymax, color=\"black\", linestyle=\"-\", zorder=zorder, alpha=alpha\n",
    ")\n",
    "axes[i].set_xlim(smin / 2, slack_max)\n",
    "axes[i].set_ylim(ymin, ymax)\n",
    "axes[i].set_xscale(\"log\")\n",
    "axes[i].annotate(\n",
    "    rf\"$s = {chosen_slack_var}$\",\n",
    "    xy=(chosen_slack_var, 0),\n",
    "    xycoords=\"data\",\n",
    "    xytext=(5, 0),\n",
    "    textcoords=\"offset points\",\n",
    "    ha=\"left\",\n",
    "    fontsize=fontdict[\"size\"],\n",
    ")\n",
    "axes[i].annotate(\n",
    "    rf\"$s > 0$\",\n",
    "    xy=(smin, ymax),\n",
    "    xycoords=\"data\",\n",
    "    xytext=(10, 5),\n",
    "    textcoords=\"offset points\",\n",
    "    ha=\"center\",\n",
    "    fontsize=fontdict[\"size\"],\n",
    ")\n",
    "axes[i].annotate(\n",
    "    rf\"$s \\leq 1$\",\n",
    "    xy=(1, ymax),\n",
    "    xycoords=\"data\",\n",
    "    xytext=(10, 5),\n",
    "    textcoords=\"offset points\",\n",
    "    ha=\"center\",\n",
    "    fontsize=fontdict[\"size\"],\n",
    ")\n",
    "axes[i].fill_between((smin / 2, smin), ymin, ymax, color=\"xkcd:light grey\")\n",
    "axes[i].annotate(\n",
    "    \"Infeasible\",\n",
    "    xy=(smin, (ymax + ymin) / 2),\n",
    "    xycoords=\"data\",\n",
    "    rotation=90,\n",
    "    xytext=(-10, 0),\n",
    "    textcoords=\"offset points\",\n",
    "    va=\"center\",\n",
    "    ha=\"right\",\n",
    "    fontsize=fontdict[\"size\"],\n",
    ")\n",
    "\n",
    "\n",
    "i += 1\n",
    "ymin, ymax = (min(o_values) * limit_pad_sclar, max(o_values) * limit_pad_sclar)\n",
    "axes[i].vlines(chosen_slack_var, ymin=ymin, ymax=ymax, color=\"black\", linestyle=\":\")\n",
    "axes[i].vlines(\n",
    "    smin, ymin=ymin, ymax=ymax, color=\"black\", linestyle=\"-\", zorder=zorder, alpha=alpha\n",
    ")\n",
    "axes[i].vlines(\n",
    "    1, ymin=ymin, ymax=ymax, color=\"black\", linestyle=\"-\", zorder=zorder, alpha=alpha\n",
    ")\n",
    "axes[i].set_xlim(smin / 2, slack_max)\n",
    "axes[i].set_ylim(ymin, ymax)\n",
    "axes[i].set_xscale(\"log\")\n",
    "# axes[i].annotate(rf'$s = {chosen_slack_var}$', xy=(chosen_slack_var, 0), xycoords='data', xytext=(5, 0), textcoords='offset points',  ha=\"left\", fontsize=fontdict[\"size\"])\n",
    "# axes[i].annotate(rf'$s > 0$', xy=(smin, ymax), xycoords='data', xytext=(10, 5), textcoords='offset points', ha=\"center\", fontsize=fontdict[\"size\"])\n",
    "# axes[i].annotate(rf'$s \\leq 1$', xy=(1, ymax), xycoords='data', xytext=(10, 5), textcoords='offset points', ha=\"center\", fontsize=fontdict[\"size\"])\n",
    "axes[i].fill_between((smin / 2, smin), ymin, ymax, color=\"xkcd:light grey\")\n",
    "axes[i].annotate(\n",
    "    \"Infeasible\",\n",
    "    xy=(smin, (ymax + ymin) / 2),\n",
    "    xycoords=\"data\",\n",
    "    rotation=90,\n",
    "    xytext=(-10, 0),\n",
    "    textcoords=\"offset points\",\n",
    "    va=\"center\",\n",
    "    ha=\"right\",\n",
    "    fontsize=fontdict[\"size\"],\n",
    ")\n",
    "\n",
    "i += 1\n",
    "\n",
    "ymin, ymax = (0, max(rxn_values) * limit_pad_sclar)\n",
    "axes[i].vlines(chosen_slack_var, ymin=ymin, ymax=ymax, color=\"black\", linestyle=\":\")\n",
    "axes[i].vlines(\n",
    "    smin, ymin=ymin, ymax=ymax, color=\"black\", linestyle=\"-\", zorder=zorder, alpha=alpha\n",
    ")\n",
    "axes[i].vlines(\n",
    "    1, ymin=ymin, ymax=ymax, color=\"black\", linestyle=\"-\", zorder=zorder, alpha=alpha\n",
    ")\n",
    "axes[i].set_xlim(smin / 2, slack_max)\n",
    "axes[i].set_ylim(ymin, ymax)\n",
    "axes[i].set_xscale(\"log\")\n",
    "# axes[i].annotate(rf'$s = {chosen_slack_var}$', xy=(chosen_slack_var, 0), xycoords='data', xytext=(5, 0), textcoords='offset points',  ha=\"left\", fontsize=fontdict[\"size\"])\n",
    "# axes[i].annotate(rf'$s > 0$', xy=(smin, ymax), xycoords='data', xytext=(10, 5), textcoords='offset points', ha=\"center\", fontsize=fontdict[\"size\"])\n",
    "# axes[i].annotate(rf'$s \\leq 1$', xy=(1, ymax), xycoords='data', xytext=(10, 5), textcoords='offset points', ha=\"center\", fontsize=fontdict[\"size\"])\n",
    "axes[i].fill_between((smin / 2, smin), ymin, ymax, color=\"xkcd:light grey\")\n",
    "axes[i].annotate(\n",
    "    \"Infeasible\",\n",
    "    xy=(smin, (ymax + ymin) / 2),\n",
    "    xycoords=\"data\",\n",
    "    rotation=90,\n",
    "    xytext=(-10, 0),\n",
    "    textcoords=\"offset points\",\n",
    "    va=\"center\",\n",
    "    ha=\"right\",\n",
    "    fontsize=fontdict[\"size\"],\n",
    ")\n",
    "\n",
    "handles, labels = axes[2].get_legend_handles_labels()\n",
    "handles_labels = dict(zip(labels, handles))\n",
    "handles_labels = {\n",
    "    k: handles_labels[k] for k in sorted(handles_labels, key=get_time_from_id)\n",
    "}\n",
    "handles, labels = (list(handles_labels.values()), list(handles_labels.keys()))\n",
    "labels = [\n",
    "    \"Day \" + \" \".join(l.replace(f\"{pcmodel.id}_\", \"\").split(\"_\")[::-1])[1:]\n",
    "    for l in labels\n",
    "]\n",
    "axes[2].legend(\n",
    "    handles=handles,\n",
    "    labels=labels,\n",
    "    ncols=1,\n",
    "    frameon=False,\n",
    "    loc=\"upper center\",\n",
    "    fontsize=\"large\",\n",
    "    bbox_to_anchor=(0.6, 0.7),\n",
    ")\n",
    "\n",
    "\n",
    "axes[0].set_ylabel(\"Relaxation budget used (%)\", fontdict=fontdict)\n",
    "axes[1].set_ylabel(\"Objective value\", fontdict=fontdict)\n",
    "axes[2].set_ylabel(\"NaKt (mmol/gDW/hr)\", fontdict=fontdict)\n",
    "\n",
    "fig.align_labels()\n",
    "if save_figures:\n",
    "    fig.savefig(\n",
    "        figures_path / f\"S2Fig_PanelD_SlackPercentDetermination.{imagetype}\",\n",
    "        transparent=transparent,\n",
    "        format=imagetype,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44927e48-3ffc-4429-b7b5-737e6e597a5b",
   "metadata": {},
   "source": [
    "### Formulate models from QP solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e22910-d710-4539-b300-193821b896ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_relaxed_models = []\n",
    "slack_value = chosen_slack_var  # Slack %\n",
    "verbose = True\n",
    "\n",
    "# In our experience, SBML/XML loads faster, but will take up to 4x more space uncompressed as compared to JSON\n",
    "ftypes = {\n",
    "    \"xml\"\n",
    "    # \"json\",\n",
    "}\n",
    "\n",
    "model_values = {}\n",
    "ftypes = set([ftypes]) if isinstance(ftypes, str) else set(ftypes)\n",
    "for sample_id, (df_qp_sol, r2, objective_value) in qp_solutions_dict.items():\n",
    "    # Create a copy of the model\n",
    "    sample_id = f\"{pcmodel.id}_{sample_id}\"\n",
    "    filenames = [\n",
    "        Path(f\"{dataset_path}/pcmodels/{sample_id}.{ftype}\") for ftype in ftypes\n",
    "    ]\n",
    "    if all([filename.exists() for filename in filenames]):\n",
    "        print(f\"Model already created for {sample_id}\")\n",
    "        continue\n",
    "    pcmodel_sample = pcmodel.copy()\n",
    "    pcmodel_sample.id = sample_id\n",
    "    for protdl in pcmodel_sample.reactions.query(\n",
    "        lambda x: isinstance(x, ProteinDilution)\n",
    "    ):\n",
    "        if protdl.id in df_qp_sol.index:\n",
    "            prot_bound = df_qp_sol.loc[protdl.id][\"Best-Fitted Proteome\"]\n",
    "        else:\n",
    "            prot_bound = 0\n",
    "        protdl.bounds = (float(prot_bound), float(prot_bound))\n",
    "    # Add the relaxation budget\n",
    "    add_relaxation_budget(pcmodel_sample, slack_value, verbose)\n",
    "    # # Store model for later use\n",
    "    # list_of_relaxed_models += [pcmodel_sample]\n",
    "    for filename in filenames:\n",
    "        # Might as well overwrite all files, especially if model needed to be regenerated anyways\n",
    "        write_cobra_model(\n",
    "            pcmodel_sample,\n",
    "            filename=filename,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f01aef-0c82-46e4-b29f-7698b19a2d80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
